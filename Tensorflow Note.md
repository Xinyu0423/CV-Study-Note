# Tensorflow Note

## Tensorflow v1
1. Define constant and set it to 36
   1. `tf.constant(36, name='y_hat')`
2. When init is run later (session.run(init))
   1. `tf.global_variables_initializer()`
3. Create a session and print the output(as shown below)
   1. `with tf.Session() as session:`
   2. `session.run(init) `
      1. Initializes the variables
   3. `print(session.run(loss))`
      1. Prints the loss
4. Multiply tf variables
   1. `tf.multiply(a,b)`
5. åœ¨åˆå§‹åŒ–variableåï¼Œéœ€è¦create a sessionå¹¶ä¸”è¿è¡Œè¿™ä¸ªsession
   1. `sess = tf.Session()`
   2. `print(sess.run(c))`
6. A placeholder is an object whose value you can specify only later. To specify values for a placeholder, you can pass in values by using a "feed dictionary" (feed_dict variable).
   1. `x = tf.placeholder(tf.int64, name = 'x')`
   2. `print(sess.run(2 * x, feed_dict = {x: 3}))`
   3. `sess.close()`
   4. Here's what's happening: When you specify the operations needed for a computation, you are telling TensorFlow how to construct a computation graph. The computation graph can have some placeholders whose values you will specify only later. Finally, when you run the session, you are telling TensorFlow to execute the computation graph.
7. Computing the sigmoid
   1. `tf.sigmoid(...)`
8. Two ways create and use session in tf
   1. Method 1:
      1. `sess = tf.Session()`
        # Run the variables initialization (if needed), run the operations
       2. result = sess.run(..., feed_dict = {...})
       3. sess.close() # Close the session
    2. Method 2:
       1. with tf.Session() as sess: 
    # run the variables initialization (if needed), run the operations
        2. result = sess.run(..., feed_dict = {...})
    # This takes care of closing the session for you :)
9. Computing the Cost
   1.  `tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)`
   2.  $$- \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log \sigma(z^{[2](i)}) + (1-y^{(i)})\log (1-\sigma(z^{[2](i)})\large )\small\tag{2}$$
   3.  Your code should input z, compute the sigmoid (to get a) and then compute the cross entropy cost  ğ½. All this can be done using one call to tf.nn.sigmoid_cross_entropy_with_logits, which computes
10. One Hot encodings
    1.  `tf.one_hot(labels, depth, axis)`
10. Initialize with zeros and onesÂ¶
    1.  Now you will learn how to initialize a vector of zeros and ones. The function you will be calling is `tf.ones()`. To initialize with zeros you could use tf.zeros() instead. These functions take in a shape and return an array of dimension shape full of zeros and ones respectively.
11. tf.contrib.layers.xavier_initializer
    1.  è¯¥å‡½æ•°è¿”å›ä¸€ä¸ªç”¨äºåˆå§‹åŒ–æƒé‡çš„åˆå§‹åŒ–ç¨‹åº â€œXavierâ€ ã€‚
    2.  è¿™ä¸ªåˆå§‹åŒ–å™¨æ˜¯ç”¨æ¥ä½¿å¾—æ¯ä¸€å±‚è¾“å‡ºçš„æ–¹å·®åº”è¯¥å°½é‡ç›¸ç­‰ã€‚
12.  tf.reduce_meanå‡½æ•°
    1.   tf.reduce_mean å‡½æ•°ç”¨äºè®¡ç®—å¼ é‡tensoræ²¿ç€æŒ‡å®šçš„æ•°è½´ï¼ˆtensorçš„æŸä¸€ç»´åº¦ï¼‰ä¸Šçš„çš„å¹³å‡å€¼ï¼Œä¸»è¦ç”¨ä½œé™ç»´æˆ–è€…è®¡ç®—tensorï¼ˆå›¾åƒï¼‰çš„å¹³å‡å€¼ã€‚
    2.   `reduce_mean(input_tensor,`
                `axis=None,`
                `keep_dims=False,`
                `name=None,`
                `reduction_indices=None)`
         1. ç¬¬ä¸€ä¸ªå‚æ•°input_tensorï¼š è¾“å…¥çš„å¾…é™ç»´çš„tensor;
         2. ç¬¬äºŒä¸ªå‚æ•°axisï¼š æŒ‡å®šçš„è½´ï¼Œå¦‚æœä¸æŒ‡å®šï¼Œåˆ™è®¡ç®—æ‰€æœ‰å…ƒç´ çš„å‡å€¼;
         3. ç¬¬ä¸‰ä¸ªå‚æ•°keep_dimsï¼šæ˜¯å¦é™ç»´åº¦ï¼Œè®¾ç½®ä¸ºTrueï¼Œè¾“å‡ºçš„ç»“æœä¿æŒè¾“å…¥tensorçš„å½¢çŠ¶ï¼Œè®¾ç½®ä¸ºFalseï¼Œè¾“å‡ºç»“æœä¼šé™ä½ç»´åº¦;
         4. ç¬¬å››ä¸ªå‚æ•°nameï¼š æ“ä½œçš„åç§°;
         5. ç¬¬äº”ä¸ªå‚æ•° reduction_indicesï¼šåœ¨ä»¥å‰ç‰ˆæœ¬ä¸­ç”¨æ¥æŒ‡å®šè½´ï¼Œå·²å¼ƒç”¨;
    3. ç±»ä¼¼å‡½æ•°è¿˜æœ‰:
       1. tf.reduce_sum ï¼šè®¡ç®—tensoræŒ‡å®šè½´æ–¹å‘ä¸Šçš„æ‰€æœ‰å…ƒç´ çš„ç´¯åŠ å’Œ;
       2. tf.reduce_max  :  è®¡ç®—tensoræŒ‡å®šè½´æ–¹å‘ä¸Šçš„å„ä¸ªå…ƒç´ çš„æœ€å¤§å€¼;
       3. tf.reduce_all :  è®¡ç®—tensoræŒ‡å®šè½´æ–¹å‘ä¸Šçš„å„ä¸ªå…ƒç´ çš„é€»è¾‘å’Œï¼ˆandè¿ç®—ï¼‰;
       4. tf.reduce_any:  è®¡ç®—tensoræŒ‡å®šè½´æ–¹å‘ä¸Šçš„å„ä¸ªå…ƒç´ çš„é€»è¾‘æˆ–ï¼ˆorè¿ç®—ï¼‰;
 13. Backward propagation
     1.  After you compute the cost function. You will create an "optimizer" object. You have to call this object along with the cost when running the tf.session. When called, it will perform an optimization on the given cost with the chosen method and learning rate.
     2.  `optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)`
     3.  To make the optimization you would do:
     4.  `_ , c = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})`
 14. Padding
     1.   Implement the following function, which pads all the images of a batch of examples X with zeros. Use np.pad. Note if you want to pad the array "a" of shape  (5,5,5,5,5)with pad = 1 for the 2nd dimension, pad = 3 for the 4th dimension and pad = 0 for the rest, you would do:
     2.   `a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))`


